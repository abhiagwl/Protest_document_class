{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.arange(151402)\n",
    "np.random.seed(0)\n",
    "indices_sample = np.random.permutation(a)\n",
    "np.random.shuffle(indices_sample)\n",
    "\n",
    "b = np.arange(1645)\n",
    "np.random.seed(0)\n",
    "indices_sample_protest = np.random.permutation(b)\n",
    "np.random.shuffle(indices_sample_protest)\n",
    "\n",
    "test_sample_indices_protest = indices_sample_protest[:165]\n",
    "val_sample_indices_protest = indices_sample_protest[165:495]\n",
    "train_sample_indices_protest = indices_sample_protest[495:]\n",
    "\n",
    "test_sample_indices = indices_sample[:15000]\n",
    "val_sample_indices = indices_sample[15000:45000]\n",
    "train_sample_indices = indices_sample[45000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = '/home/abhinav/data/GLOVE_DATA/glove.6B'\n",
    "TEXT_DATA_DIR = '/home/abhinav/data/full_text'\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 50000\n",
    "EMBEDDING_DIM = 200\n",
    "VALIDATION_SPLIT = 0.3\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ': done')\n",
      "(10000, ': done')\n",
      "(20000, ': done')\n",
      "(30000, ': done')\n",
      "(40000, ': done')\n",
      "(50000, ': done')\n",
      "(60000, ': done')\n",
      "(70000, ': done')\n",
      "(80000, ': done')\n",
      "(90000, ': done')\n",
      "(100000, ': done')\n",
      "(110000, ': done')\n",
      "(120000, ': done')\n",
      "(130000, ': done')\n",
      "(140000, ': done')\n",
      "(150000, ': done')\n",
      "151402\n"
     ]
    }
   ],
   "source": [
    "TEXT_DATA_DIR = \"/home/abhinav/data/full_text\"\n",
    "text_neg_val = []\n",
    "text_neg_train = []\n",
    "text_neg_test = []\n",
    "count = 0\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR+\"/not_protest\")):\n",
    "    path = os.path.join(TEXT_DATA_DIR+'/not_protest', name)\n",
    "    f = open(path)\n",
    "    t = f.read()\n",
    "    t = t.split(\"</DOC>\")[:-1]\n",
    "    for i,docs in enumerate(t):\n",
    "        t[i] = re.sub(r\"((\\<.*\\>)|(\\d+)|(LEAD:(.*)\\n))\",\"\",t[i])\n",
    "        t[i] = [j for j in t[i].lower().split() if j not in stop]\n",
    "        t[i] = \" \".join(t[i])\n",
    "        t[i] = \" \".join(text_to_word_sequence(t[i]))\n",
    "        if (count%10000)==0:\n",
    "            print (count,\": done\")\n",
    "        if count in test_sample_indices:\n",
    "            text_neg_test.append(t[i])\n",
    "        elif count in val_sample_indices:\n",
    "            text_neg_val.append(t[i])\n",
    "        elif count in train_sample_indices:\n",
    "            text_neg_train.append(t[i])\n",
    "        count=count+1\n",
    "    f.close()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1645\n"
     ]
    }
   ],
   "source": [
    "TEXT_DATA_DIR = \"/home/abhinav/data/full_text\"\n",
    "\n",
    "text_pos_val = []\n",
    "text_pos_train  = []\n",
    "text_pos_test = []\n",
    "count = 0\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR+\"/protest\")):\n",
    "    path = os.path.join(TEXT_DATA_DIR+'/protest', name)\n",
    "    f = open(path)\n",
    "    t = f.read()\n",
    "    t = t.split(\"</DOC>\")[:-1]\n",
    "    for i,docs in enumerate(t):\n",
    "        t[i] = re.sub(r\"((\\<.*\\>)|(\\d+)|(LEAD:(.*)\\n))\",\"\",t[i])\n",
    "        t[i] = [j for j in t[i].lower().split() if j not in stop]\n",
    "        t[i] = \" \".join(t[i])\n",
    "        t[i] = \" \".join(text_to_word_sequence(t[i]))\n",
    "        if count in test_sample_indices_protest:\n",
    "            text_pos_test.append(t[i])\n",
    "        elif count in val_sample_indices_protest:\n",
    "            text_pos_val.append(t[i])\n",
    "        elif count in train_sample_indices_protest:\n",
    "            text_pos_train.append(t[i])\n",
    "        count= count +1\n",
    "    f.close()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data  =  text_neg_train + text_pos_train \n",
    "val_data  =   text_neg_val + text_pos_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_data  =  text_neg_train + text_pos_train + text_neg_val + text_pos_val\n",
    "\n",
    "label_neg_train = np.zeros((len(text_neg_train),),dtype=int)\n",
    "label_pos_train = np.ones((len(text_pos_train),),dtype=int)\n",
    "labels_train = np.append(label_neg_train,label_pos_train)\n",
    "\n",
    "label_neg_val = np.zeros((len(text_neg_val),),dtype=int)\n",
    "label_pos_val = np.ones((len(text_pos_val),),dtype=int)\n",
    "labels_val = np.append(label_neg_val,label_pos_val)\n",
    "\n",
    "MAX_NB_WORDS =50000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape of data tensor:', (137882, 1000))\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "\n",
    "x_train = data[:len(text_neg_train)+len(text_pos_train)]\n",
    "x_val = data[len(text_neg_train)+len(text_pos_train):]\n",
    "\n",
    "y_train = to_categorical(np.asarray(labels_train))\n",
    "y_val = to_categorical(np.asarray(labels_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FIFTH_MOD_DIR = \"/home/abhinav/data/test_models/\"\n",
    "MOD_DIR = FIFTH_MOD_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle([x_val,y_val],MOD_DIR+\"validation_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle([x_train,y_train],MOD_DIR+\"train_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(text_pos_test)\n",
    "x_test_pos = pad_sequences(test_seq,maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "y_test_positives = np.ones((len(text_pos_test),))\n",
    "temp = np.append(np.zeros((1,)),y_test_positives)\n",
    "y_test_positives = to_categorical(temp)[1:]\n",
    "\n",
    "test_seq_ = tokenizer.texts_to_sequences(text_neg_test)\n",
    "x_test_neg = pad_sequences(test_seq_,maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "y_test_negatives = np.zeros((len(text_neg_test),))\n",
    "temp = np.append(np.ones((1,)),y_test_negatives)\n",
    "y_test_negatives = to_categorical(temp)[1:]\n",
    "\n",
    "x_test = np.append(x_test_neg,x_test_pos,axis=0)\n",
    "y_test = to_categorical(np.append(np.zeros((len(text_neg_test),)),np.ones((len(text_pos_test),))))\n",
    "pd.to_pickle(obj=[x_test,y_test], path=MOD_DIR + \"test_dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_neg = data[:len(te)]\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(x_train_neg)\n",
    "\n",
    "x_train_neg_10 = x_train_neg[:12000]\n",
    "x_train_neg_1 = x_train_neg[:1200]\n",
    "\n",
    "x_train_pos = data[106402:106402+1150]\n",
    "\n",
    "x_train = np.append(x_train_neg_10,x_train_pos,axis = 0)\n",
    "y_train = to_categorical(np.append(np.zeros(12000,),np.ones(1150,),axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle([x_train,y_train],MOD_DIR+'train_data_10ratio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle(tokenizer,MOD_DIR+'tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "embeddings_index = pd.read_pickle('/home/abhinav/data/fourth_model/embedding_hash_200d')\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle(embedding_matrix,MOD_DIR + \"embedding_matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle(obj=[text_neg_train,text_pos_train,text_neg_val,text_pos_val,text_neg_test,text_pos_test],path=\"/home/abhinav/data/second_model/list_for_alen\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
