{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "(13150, 1000) train sequences\n",
      "(13150, 2) test sequences\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "BASE_DIR = ''\n",
    "GLOVE_DIR = '/home/abhinav/data/GLOVE_DATA/glove.6B'\n",
    "TEXT_DATA_DIR = '/home/abhinav/data/full_text'\n",
    "THIRD_MOD_DIR = \"/home/abhinav/data/third_model/\"\n",
    "SECOND_MOD_DIR = \"/home/abhinav/data/second_model/\"\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 50000\n",
    "EMBEDDING_DIM = 200\n",
    "FOURTH_MOD_DIR = \"/home/abhinav/data/fourth_model/\"\n",
    "\n",
    "print('Loading data...')\n",
    "[x_train, y_train] = pd.read_pickle(FOURTH_MOD_DIR + \"train_data_10ratio\") \n",
    "\n",
    "[x_test, y_test] = pd.read_pickle(FOURTH_MOD_DIR + \"test_dataset\") \n",
    "\n",
    "[x_val, y_val ] = pd.read_pickle(FOURTH_MOD_DIR + \"validation_dataset\")\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D,merge, MaxPooling1D, Embedding,Merge,  Dropout\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers.merge import Add,Concatenate,Dot\n",
    "\n",
    "num_words = MAX_NB_WORDS\n",
    "\n",
    "embedding_matrix = pd.read_pickle(\"/home/abhinav/data/fourth_model/embedding_matrix_for_selected_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Metrics(keras.callbacks.Callback):\n",
    "    def __init__(self,filepath):\n",
    "        self.filepath = filepath\n",
    "        self.best = -np.Inf\n",
    "    def on_epoch_end(self, epoch,batch, logs={}):\n",
    "        predict = self.model.predict(self.validation_data[0],batch_size = 512)\n",
    "        predict = prob_to_label(predict)\n",
    "        targ = self.validation_data[1]\n",
    "        self.f1s=f1(targ, predict)\n",
    "        \"\"\"if self.f1s > self.best:\n",
    "            self.best = self.f1s\n",
    "            print('Epoch %05d: improved f1 to %0.5f,'\n",
    "                                  ' saving model to %s'\n",
    "                                  % (epoch, self.best, self.filepath))\n",
    "            self.model.save(self.filepath,overwrite =True)\"\"\"\n",
    "        print (\"\\nWeighted F1 score found on Validation dataset : \" ,self.f1s)\n",
    "        return\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "def f1(y_true, y_pred):\n",
    "    return precision_recall_fscore_support(y_true = y_true, y_pred = y_pred)\n",
    "ndm = keras.optimizers.Nadam(lr = 0.001)    \n",
    "\n",
    "def prob_to_label(array):\n",
    "    for a in array:\n",
    "        if a[0] > a[1] :\n",
    "            a[0] = 1\n",
    "            a[1] = 0\n",
    "        elif a[0] < a[1]:\n",
    "            a[0] = 0\n",
    "            a[1] = 1\n",
    "        else:\n",
    "            ind = np.random.randint(2)\n",
    "            a[ind]= 1\n",
    "            a[int(1-ind)] = 0\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 1000)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 1000, 200)     10000000    input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)                (None, 998, 32)       19232       embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)                (None, 997, 32)       25632       embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)                (None, 996, 32)       32032       embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)   (None, 332, 32)       0           conv1d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)   (None, 249, 32)       0           conv1d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)   (None, 199, 32)       0           conv1d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "merge_2 (Merge)                  (None, 780, 32)       0           max_pooling1d_5[0][0]            \n",
      "                                                                   max_pooling1d_6[0][0]            \n",
      "                                                                   max_pooling1d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)                (None, 776, 32)       5152        merge_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)   (None, 77, 32)        0           conv1d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 2464)          0           max_pooling1d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 128)           315520      flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 2)             258         dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 10,397,826\n",
      "Trainable params: 10,397,826\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhinav/miniconda2/envs/tensorflow/lib/python2.7/site-packages/ipykernel_launcher.py:20: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "# applying a more complex convolutional approach\n",
    "convs = []\n",
    "filter_sizes = [3,4,5]\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "for fsz in filter_sizes:\n",
    "    l_conv = Conv1D(filters=32,kernel_size=fsz,activation='relu')(embedded_sequences)\n",
    "    l_pool = MaxPooling1D(fsz)(l_conv)\n",
    "    convs.append(l_pool)\n",
    "    \n",
    "l_merge = Merge(mode='concat', concat_axis=1)(convs)\n",
    "l_cov1= Conv1D(32, 5, activation='relu')(l_merge)\n",
    "l_pool1 = MaxPooling1D(10)(l_cov1)\n",
    "l_flat = Flatten()(l_pool1)\n",
    "l_dense = Dense(128, activation='relu')(l_flat)\n",
    "preds = Dense(2, activation='softmax')(l_dense)\n",
    "\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=ndm,\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13150 samples, validate on 15165 samples\n",
      "Epoch 1/10\n",
      "13056/13150 [============================>.] - ETA: 1s - loss: 0.7501 - acc: 0.8683Weighted F1 score found on Validation dataset :  (array([ 0.99965956,  0.02549976]), array([ 0.58726667,  0.98181818]), array([ 0.73987905,  0.0497085 ]), array([15000,   165]))\n",
      "13150/13150 [==============================] - 330s - loss: 0.7495 - acc: 0.8684 - val_loss: 0.7727 - val_acc: 0.5916\n",
      "Epoch 2/10\n",
      "13056/13150 [============================>.] - ETA: 1s - loss: 0.3882 - acc: 0.8996Weighted F1 score found on Validation dataset :  (array([ 0.99829415,  0.08442331]), array([ 0.89733333,  0.86060606]), array([ 0.94512516,  0.15376286]), array([15000,   165]))\n",
      "13150/13150 [==============================] - 323s - loss: 0.3875 - acc: 0.9000 - val_loss: 0.2293 - val_acc: 0.8969\n",
      "Epoch 3/10\n",
      "13056/13150 [============================>.] - ETA: 1s - loss: 0.2222 - acc: 0.9411Weighted F1 score found on Validation dataset :  (array([ 0.99591639,  0.14783821]), array([ 0.95926667,  0.64242424]), array([ 0.97724803,  0.24036281]), array([15000,   165]))\n",
      "13150/13150 [==============================] - 321s - loss: 0.2219 - acc: 0.9411 - val_loss: 0.1088 - val_acc: 0.9558\n",
      "Epoch 4/10\n",
      "13056/13150 [============================>.] - ETA: 1s - loss: 0.1150 - acc: 0.9737Weighted F1 score found on Validation dataset :  (array([ 0.99510817,  0.14439324]), array([ 0.96286667,  0.56969697]), array([ 0.97872196,  0.23039216]), array([15000,   165]))\n",
      "13150/13150 [==============================] - 318s - loss: 0.1145 - acc: 0.9739 - val_loss: 0.1361 - val_acc: 0.9586\n",
      "Epoch 5/10\n",
      "13056/13150 [============================>.] - ETA: 1s - loss: 0.0869 - acc: 0.9813Weighted F1 score found on Validation dataset :  (array([ 0.995493 ,  0.1345895]), array([ 0.95713333,  0.60606061]), array([ 0.97593637,  0.22026432]), array([15000,   165]))\n",
      "13150/13150 [==============================] - 323s - loss: 0.0866 - acc: 0.9814 - val_loss: 0.1543 - val_acc: 0.9533\n",
      "Epoch 6/10\n",
      " 2048/13150 [===>..........................] - ETA: 153s - loss: 0.0427 - acc: 0.9917"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e92ca927a660>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0.\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m          callbacks = [metrics])\n\u001b[0m",
      "\u001b[0;32m/home/abhinav/miniconda2/envs/tensorflow/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1505\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/abhinav/miniconda2/envs/tensorflow/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/abhinav/miniconda2/envs/tensorflow/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/abhinav/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/abhinav/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/abhinav/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/abhinav/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/abhinav/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.set_weights(initial_weights)\n",
    "metrics = Metrics(FOURTH_MOD_DIR + \"none\")\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          class_weight = {0. : 1, 1. : 5},\n",
    "validation_data=[x_test, y_test],\n",
    "         callbacks = [metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13150 samples, validate on 15165 samples\n",
      "Epoch 1/6\n",
      "13056/13150 [============================>.] - ETA: 1s - loss: 0.9387 - acc: 0.8357"
     ]
    }
   ],
   "source": [
    "model.set_weights(initial_weights)\n",
    "metrics = Metrics(FOURTH_MOD_DIR + \"none\")\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=6,\n",
    "          class_weight = {0. : 1, 1. : 8},\n",
    "validation_data=[x_test, y_test],\n",
    "         callbacks = [metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.set_weights(initial_weights)\n",
    "metrics = Metrics(FOURTH_MOD_DIR + \"none\")\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=6,\n",
    "          class_weight = {0. : 1, 1. : 10},\n",
    "validation_data=[x_test, y_test],\n",
    "         callbacks = [metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.set_weights(initial_weights)\n",
    "metrics = Metrics(FOURTH_MOD_DIR + \"none\")\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=6,\n",
    "          class_weight = {0. : 1, 1. : 12},\n",
    "validation_data=[x_test, y_test],\n",
    "         callbacks = [metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.set_weights(initial_weights)\n",
    "metrics = Metrics(FOURTH_MOD_DIR + \"none\")\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=6,\n",
    "          class_weight = {0. : 1, 1. : 15},\n",
    "validation_data=[x_test, y_test],\n",
    "         callbacks = [metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.set_weights(initial_weights)\n",
    "metrics = Metrics(FOURTH_MOD_DIR + \"none\")\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=6,\n",
    "          class_weight = {0. : 1, 1. : 5},\n",
    "validation_data=[x_test, y_test],\n",
    "         callbacks = [metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.set_weights(initial_weights)\n",
    "metrics = Metrics(FOURTH_MOD_DIR + \"none\")\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=6,\n",
    "          class_weight = {0. : 1, 1. : 8},\n",
    "validation_data=[x_test, y_test],\n",
    "         callbacks = [metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.set_weights(initial_weights)\n",
    "metrics = Metrics(FOURTH_MOD_DIR + \"none\")\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=6,\n",
    "          class_weight = {0. : 1, 1. : 10},\n",
    "validation_data=[x_test, y_test],\n",
    "         callbacks = [metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.set_weights(initial_weights)\n",
    "metrics = Metrics(FOURTH_MOD_DIR + \"none\")\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=10,\n",
    "          class_weight = {0. : 1, 1. : 12},\n",
    "validation_data=[x_test, y_test],\n",
    "         callbacks = [metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.set_weights(initial_weights)\n",
    "metrics = Metrics(FOURTH_MOD_DIR + \"none\")\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=10,\n",
    "          class_weight = {0. : 1, 1. : 15},\n",
    "validation_data=[x_test, y_test],\n",
    "         callbacks = [metrics])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
