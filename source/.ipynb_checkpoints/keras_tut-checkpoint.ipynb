{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import keras.regularizers\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\"\"\"\n",
    "pd.to_pickle(obj = text_neg,path=\"/home/abhinav/data/saved_samples\")\n",
    "pd.to_pickle(obj = text_pos,path = \"/home/abhinav/data/saved_postives\")\n",
    "pd.to_pickle(obj = embeddings_index,path = \"/home/abhinav/data/first_model/embeddings_index_dict\")\n",
    "pd.to_pickle(obj = tokenizer, path = \"/home/abhinav/data/first_model/tokenizer\")\n",
    "\"\"\"\n",
    "\n",
    "text_neg = pd.read_pickle(\"/home/abhinav/data/saved_samples\")\n",
    "text_pos = pd.read_pickle(\"/home/abhinav/data/saved_postives\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = '/home/abhinav/data/GLOVE_DATA/glove.6B'\n",
    "TEXT_DATA_DIR = '/home/abhinav/data/full_text'\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 50000\n",
    "EMBEDDING_DIM = 200\n",
    "VALIDATION_SPLIT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors.')\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.200d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "#embeddings_index = pd.read_pickle(\"/home/abhinav/data/first_model/embeddings_index_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle(embeddings_index,\"/home/abhinav/data/fourth_model/embedding_hash_200d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.arange(151402)\n",
    "np.random.seed(0)\n",
    "indices_sample = np.random.permutation(a)\n",
    "np.random.shuffle(indices_sample)\n",
    "\n",
    "b = np.arange(1645)\n",
    "np.random.seed(10)\n",
    "indices_sample_protest = np.random.permutation(b)\n",
    "np.random.shuffle(indices_sample_protest)\n",
    "\n",
    "test_sample_indices_protest = indices_sample_protest[:165]\n",
    "val_sample_indices_protest = indices_sample_protest[165:495]\n",
    "train_sample_indices_protest = indices_sample_protest[495:]\n",
    "\n",
    "test_sample_indices = indices_sample[:15000]\n",
    "val_sample_indices = indices_sample[15000:45000]\n",
    "train_sample_indices = indices_sample[45000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : done\n",
      "10000 : done\n",
      "20000 : done\n",
      "30000 : done\n",
      "40000 : done\n",
      "50000 : done\n",
      "60000 : done\n",
      "70000 : done\n",
      "80000 : done\n",
      "90000 : done\n",
      "100000 : done\n",
      "110000 : done\n",
      "120000 : done\n",
      "130000 : done\n",
      "140000 : done\n",
      "150000 : done\n",
      "151402\n"
     ]
    }
   ],
   "source": [
    "text_neg_val = []\n",
    "text_neg_train = []\n",
    "text_neg_test = []\n",
    "count = 0\n",
    "count1 = 0\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR+\"/not_protest\")):\n",
    "    path = os.path.join(TEXT_DATA_DIR+'/not_protest', name)\n",
    "    f = open(path)\n",
    "    t = f.read()\n",
    "    t = t.split(\"</DOC>\")[:-1]\n",
    "    for i,docs in enumerate(t):\n",
    "        t[i] = re.sub(r\"((\\<.*\\>)|(\\d+)|(LEAD:(.*)\\n))\",\"\",t[i])\n",
    "        if (count%10000)==0:\n",
    "            print (count,\": done\")\n",
    "        if count in test_sample_indices:\n",
    "            text_neg_test.append(t[i])\n",
    "        elif count in val_sample_indices:\n",
    "            text_neg_val.append(t[i])\n",
    "        elif count in train_sample_indices:\n",
    "            text_neg_train.append(t[i])\n",
    "        count=count+1\n",
    "    f.close()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1645\n"
     ]
    }
   ],
   "source": [
    "text_pos_val = []\n",
    "text_pos_train  = []\n",
    "text_pos_test = []\n",
    "count = 0\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR+\"/protest\")):\n",
    "    path = os.path.join(TEXT_DATA_DIR+'/protest', name)\n",
    "    f = open(path)\n",
    "    t = f.read()\n",
    "    t = t.split(\"</DOC>\")[:-1]\n",
    "    for i,docs in enumerate(t):\n",
    "        t[i] = re.sub(r\"((\\<.*\\>)|(\\d+)|(LEAD:(.*)\\n))\",\"\",t[i])\n",
    "        if count in test_sample_indices_protest:\n",
    "            text_pos_test.append(t[i])\n",
    "        elif count in val_sample_indices_protest:\n",
    "            text_pos_val.append(t[i])\n",
    "        elif count in train_sample_indices_protest:\n",
    "            text_pos_train.append(t[i])\n",
    "        count= count +1\n",
    "    f.close()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data  =  text_neg_train + text_pos_train + text_neg_val + text_pos_val\n",
    "\n",
    "label_neg_train = np.zeros((len(text_neg_train),),dtype=int)\n",
    "label_pos_train = np.ones((len(text_pos_train),),dtype=int)\n",
    "labels_train = np.append(label_neg_train,label_pos_train)\n",
    "\n",
    "label_neg_val = np.zeros((len(text_neg_val),),dtype=int)\n",
    "label_pos_val = np.ones((len(text_pos_val),),dtype=int)\n",
    "labels_val = np.append(label_neg_val,label_pos_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS =50000\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "tokenizer = pd.read_pickle(\"/home/abhinav/data/fourth_model/tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data = pd.read_pickle(\"/home/abhinav/data/fourth_model/data_in_sequence_form\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 338446 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle(obj=tokenizer,path = \"/home/abhinav/data/fourth_model/tokenizer\")\n",
    "pd.to_pickle(obj=data, path = \"/home/abhinav/data/fourth_model/data_in_sequence_form\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (137882, 1000)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data tensor:', data.shape)\n",
    "\n",
    "x_train = data[:len(text_neg_train)+len(text_pos_train)]\n",
    "x_val = data[len(text_neg_train)+len(text_pos_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(np.asarray(labels_train))\n",
    "y_val = to_categorical(np.asarray(labels_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fourth_model_dir = \"/home/abhinav/data/fourth_model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(obj=[x_val, y_val], path= fourth_model_dir + \"validation_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to convert downsample the non protest data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_neg = data[:len(text_neg_train)]\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(x_train_neg)\n",
    "\n",
    "\n",
    "x_train_neg_10 = x_train_neg[:12000]\n",
    "x_train_neg_1 = x_train_neg[:1200]\n",
    "\n",
    "\n",
    "\n",
    "x_train_pos = data[len(text_neg_train):len(text_neg_train)+len(text_pos_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle([x_train_pos,y_train_pos],\"/home/abhinav/data/second_model/train_positive_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to create the traininig data after downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = np.append(x_train_neg_10,x_train_pos,axis = 0)\n",
    "y_train = to_categorical(np.append(np.zeros(12000,),np.ones(len(text_pos_train),),axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle([x_train,y_train],\"/home/abhinav/data/fourth_model/train_data_10ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.to_pickle(obj=[x_train,y_train],path=second_model_dir+\"training_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to get only the protest examples in the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_positives = data[-330:]\n",
    "y_val_positives = np.ones((330,))\n",
    "temp = np.append(np.zeros((1,)),y_val_positives)\n",
    "y_val_positives = to_categorical(temp)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle([x_val_positives,y_val_positives],path = \"/home/abhinav/data/second_model/val_positive_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to get only the non protest examples of the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_val_negatives = data[-30330:-330]\n",
    "y_val_negatives = np.zeros((30000,))\n",
    "temp_ = np.append(np.ones((1,)),y_val_negatives)\n",
    "y_val_negatives = to_categorical(temp_)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(obj = [x_val_negatives,y_val_negatives] , path=\"/home/abhinav/data/fourth_model/val_negative_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to get the positives from test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(text_pos_test)\n",
    "x_test_pos = pad_sequences(test_seq,maxlen = MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_positives = np.ones((len(text_pos_test),))\n",
    "temp = np.append(np.zeros((1,)),y_test_positives)\n",
    "y_test_positives = to_categorical(temp)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to get the negatives from the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_seq_ = tokenizer.texts_to_sequences(text_neg_test)\n",
    "x_test_neg = pad_sequences(test_seq_,maxlen = MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_negatives = np.zeros((len(text_neg_test),))\n",
    "temp = np.append(np.ones((1,)),y_test_negatives)\n",
    "y_test_negatives = to_categorical(temp)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.append(x_test_neg,x_test_pos,axis=0)\n",
    "y_test = to_categorical(np.append(np.zeros((len(text_neg_test),)),np.ones((len(text_pos_test),))))\n",
    "pd.to_pickle(obj=[x_test,y_test], path=fourth_model_dir + \"test_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.to_pickle(embedding_matrix, fourth_model_dir + \"embedding_matrix_for_selected_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indice_shuffler = np.arange(x_train.shape[0])\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(indice_shuffler)\n",
    "x_train = x_train[indice_shuffler]\n",
    "y_train = y_train[indice_shuffler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13150, 1000)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indice_shuffler_ = np.arange(x_val.shape[0])\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(indice_shuffler_)\n",
    "x_val = x_val[indice_shuffler_]\n",
    "y_val = y_val[indice_shuffler_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pretrained = False\n",
    "if pretrained:\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Metrics(keras.callbacks.Callback):\n",
    "    def __init__(self,filepath):\n",
    "        self.filepath = filepath\n",
    "        self.best = -np.Inf\n",
    "    def on_epoch_end(self, epoch,batch, logs={}):\n",
    "        predict = to_categorical(self.model.predict_classes(self.validation_data[0],batch_size = 512))\n",
    "        targ = self.validation_data[1]\n",
    "        self.f1s=f1(targ, predict)\n",
    "        \"\"\"if self.f1s > self.best:\n",
    "            self.best = self.f1s\n",
    "            print('Epoch %05d: improved f1 to %0.5f,'\n",
    "                                  ' saving model to %s'\n",
    "                                  % (epoch, self.best, self.filepath))\n",
    "            self.model.save(self.filepath,overwrite =True)\"\"\"\n",
    "        print (\"Weighted F1 score found on Validation dataset : \" ,self.f1s)\n",
    "        return\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "def f1(y_true, y_pred):\n",
    "    return precision_recall_fscore_support(y_true = y_true, y_pred = y_pred)\n",
    "ndm = keras.optimizers.Nadam(lr = 0.001)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = Sequential()\n",
    "model_.add(Embedding(num_words,EMBEDDING_DIM,input_length=MAX_SEQUENCE_LENGTH))\n",
    "model_.add(Conv1D(128, 5, activation='relu'))\n",
    "model_.add(MaxPooling1D(5))\n",
    "model_.add(Conv1D(128, 5, activation='relu'))\n",
    "model_.add(MaxPooling1D(5))\n",
    "model_.add(Conv1D(128, 5, activation='relu'))\n",
    "model_.add(MaxPooling1D(35))\n",
    "model_.add(Flatten())\n",
    "model_.add(Dense(128,activation=\"relu\"))\n",
    "model_.add(Dense(2,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_.compile(loss = \"binary_crossentropy\",metrics = [\"acc\"], \n",
    "                       optimizer=ndm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_hm_weights = homemade_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "homemade_model.set_weights(initial_hm_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13150 samples, validate on 15165 samples\n",
      "Epoch 1/4\n",
      "  256/13150 [..............................] - ETA: 238s - loss: 1.0936 - acc: 0.7520"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-13ff04c64e35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                   \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0.\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                   \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                    \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                   )\n",
      "\u001b[0;32m/home/abhinav/miniconda2/envs/tensorflow/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    868\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/abhinav/miniconda2/envs/tensorflow/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1505\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1507\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/abhinav/miniconda2/envs/tensorflow/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/abhinav/miniconda2/envs/tensorflow/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/abhinav/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/abhinav/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/abhinav/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/abhinav/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/abhinav/miniconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "metrics = Metrics(fourth_model_dir + \"none\")\n",
    "homemade_model.fit(x_train,y_train\n",
    "                  ,batch_size = 128,\n",
    "                  epochs = 4,\n",
    "                  class_weight = {0. :1, 1. : 7},\n",
    "                  validation_data = [x_test,y_test],\n",
    "                   callbacks = [metrics]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics = Metrics(fourth_model_dir + \"none\")\n",
    "homemade_model.fit(x_train,y_train\n",
    "                  ,batch_size = 128,\n",
    "                  epochs = 4,\n",
    "                  class_weight = {0. :1, 1. : 8},\n",
    "                  validation_data = [x_test,y_test],\n",
    "                   callbacks = [metrics]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11228 samples, validate on 30350 samples\n",
      "Epoch 1/3\n",
      "30350/30350 [==============================] - 106s   \n",
      "Epoch 00000: improved f1 to 0.88560, saving model to /home/abhinav/data/second_model/lets_try_again_to_save\n",
      "Weighted F1 score found on Validation dataset :  0.885603836893\n",
      "11228/11228 [==============================] - 328s - loss: 0.8228 - acc: 0.7879 - val_loss: 0.3938 - val_acc: 0.8116\n",
      "Epoch 2/3\n",
      "30350/30350 [==============================] - 107s   \n",
      "Epoch 00001: improved f1 to 0.95612, saving model to /home/abhinav/data/second_model/lets_try_again_to_save\n",
      "Weighted F1 score found on Validation dataset :  0.956122352843\n",
      "11228/11228 [==============================] - 322s - loss: 0.3977 - acc: 0.9057 - val_loss: 0.1649 - val_acc: 0.9329\n",
      "Epoch 3/3\n",
      "30350/30350 [==============================] - 110s   \n",
      "Weighted F1 score found on Validation dataset :  0.948642180024\n",
      "11228/11228 [==============================] - 332s - loss: 0.1964 - acc: 0.9559 - val_loss: 0.2205 - val_acc: 0.9195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb7d994a610>"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homemade_model.fit(x_train,y_train\n",
    "                  ,batch_size = 128,\n",
    "                  epochs =3,\n",
    "                  class_weight = {0. :1, 1. : 8},\n",
    "                  validation_data = [x_val,y_val],\n",
    "                   callbacks = [metrics]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/30000 [==============================] - 173s   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7458493810017903, 0.11363333338101705]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homemade_model.evaluate(x_val_negatives,y_val_negatives,batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165/165 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.072426959872245789, 1.0]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homemade_model.evaluate(x_test_pos,y_test_positives,batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000 [==============================] - 85s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7384219600677491, 0.11386666659514109]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homemade_model.evaluate(x_test_neg,y_test_negatives,batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "homemade_model1 = load_model(second_model_dir+\"lets_try_again_to_save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.85618859529495239, 0.691428542137146]"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homemade_model1.evaluate(x_test_pos,y_test_positives,batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 1s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.88645219734736846, 0.69714285884584704]"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homemade_model1.evaluate(x_val_positives,y_val_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/30000 [==============================] - 105s   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15647533753712972, 0.93570000009536747]"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homemade_model1.evaluate(x_val_negatives,y_val_negatives,batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.],\n",
       "       [ 0.,  1.]])"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(homemade_model.predict_classes(x_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='binary_crossentropy',metrics=[\"acc\"]\n",
    "              ,optimizer=ndm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.set_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11228 samples, validate on 30350 samples\n",
      "Epoch 1/2\n",
      "11136/11228 [============================>.] - ETA: 1s - loss: 1.2254 - acc: 0.6608Epoch 00000: improved f1 to 0.98274, saving model to /home/abhinav/data/second_model/trying_to_save_model\n",
      "Weighted F1 score found on Validation dataset :  0.982735252488\n",
      "11228/11228 [==============================] - 334s - loss: 1.2240 - acc: 0.6630 - val_loss: 0.6886 - val_acc: 0.9885\n",
      "Epoch 2/2\n",
      "11136/11228 [============================>.] - ETA: 1s - loss: 1.2226 - acc: 0.8909Weighted F1 score found on Validation dataset :  0.982735252488\n",
      "11228/11228 [==============================] - 323s - loss: 1.2239 - acc: 0.8906 - val_loss: 0.6867 - val_acc: 0.9885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb7da9aef90>"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = Metrics(second_model_dir + \"trying_to_save_model\")\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=2,\n",
    "          class_weight = { 0. : 1, 1. : 8 },\n",
    "          validation_data=(x_val, y_val),\n",
    "          callbacks =[metrics]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98273525248788973"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98273525248788973"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(x_val,y_val,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "second_model_dir = \"/home/abhinav/data/second_model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = load_model(second_model_dir + \"saved_model_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 1s\n",
      "[0.57591825723648071, 0.96285712718963623]\n",
      "30000/30000 [==============================] - 102s   \n",
      "[0.72436426213582361, 0.32140000011126202]\n"
     ]
    }
   ],
   "source": [
    "print (model1.evaluate(x_val_positives,y_val_positives,batch_size=512))\n",
    "print (model1.evaluate(x_val_negatives,y_val_negatives,batch_size=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.69980263710021973, 0.0]"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_val_positives,y_val_positives,batch_size=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/30000 [==============================] - 93s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.68653606402079259, 1.0]"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_val_negatives,y_val_negatives,batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.69980294534138265, 0.0]"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_pos,y_test_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test_neg,y_test_negatives,batch_size = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate(x_val[:100],y_val[:100],model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "del model\n",
    "model = load_model(\"/home/abhinav/data/second_model/the_best_model_with_trained_word_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"/home/abhinav/data/second_model/test_from_news\")\n",
    "\n",
    "test_news = f.read()\n",
    "\n",
    "test_news = test_news.split(\"</DOC>\")\n",
    "\n",
    "x_news = tokenizer.texts_to_sequences(test_news)\n",
    "\n",
    "x_news = pad_sequences(x_news,maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2799862 ,  0.7200138 ],\n",
       "       [ 0.53245479,  0.46754521]], dtype=float32)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=[X_test,y_test], \n",
    "       verbose=1, callbacks=[metrics])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
