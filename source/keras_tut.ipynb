{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import keras.regularizers\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\"\"\"\n",
    "pd.to_pickle(obj = text_neg,path=\"/home/abhinav/data/saved_samples\")\n",
    "pd.to_pickle(obj = text_pos,path = \"/home/abhinav/data/saved_postives\")\n",
    "pd.to_pickle(obj = embeddings_index,path = \"/home/abhinav/data/first_model/embeddings_index_dict\")\n",
    "pd.to_pickle(obj = tokenizer, path = \"/home/abhinav/data/first_model/tokenizer\")\n",
    "\"\"\"\n",
    "\n",
    "text_neg = pd.read_pickle(\"/home/abhinav/data/saved_samples\")\n",
    "text_pos = pd.read_pickle(\"/home/abhinav/data/saved_postives\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = '/home/abhinav/data/GLOVE_DATA/glove.6B'\n",
    "TEXT_DATA_DIR = '/home/abhinav/data/full_text'\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 50000\n",
    "EMBEDDING_DIM = 200\n",
    "VALIDATION_SPLIT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors.')\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.200d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "#embeddings_index = pd.read_pickle(\"/home/abhinav/data/first_model/embeddings_index_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle(embeddings_index,\"/home/abhinav/data/fourth_model/embedding_hash_200d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index = pd.read_pickle(\"/home/abhinav/data/fourth_model/embedding_hash_200d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.arange(151402)\n",
    "np.random.seed(0)\n",
    "indices_sample = np.random.permutation(a)\n",
    "np.random.shuffle(indices_sample)\n",
    "\n",
    "b = np.arange(1645)\n",
    "np.random.seed(10)\n",
    "indices_sample_protest = np.random.permutation(b)\n",
    "np.random.shuffle(indices_sample_protest)\n",
    "\n",
    "test_sample_indices_protest = indices_sample_protest[:165]\n",
    "val_sample_indices_protest = indices_sample_protest[165:495]\n",
    "train_sample_indices_protest = indices_sample_protest[495:]\n",
    "\n",
    "test_sample_indices = indices_sample[:15000]\n",
    "val_sample_indices = indices_sample[15000:45000]\n",
    "train_sample_indices = indices_sample[45000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : done\n",
      "10000 : done\n",
      "20000 : done\n",
      "30000 : done\n",
      "40000 : done\n",
      "50000 : done\n",
      "60000 : done\n",
      "70000 : done\n",
      "80000 : done\n",
      "90000 : done\n",
      "100000 : done\n",
      "110000 : done\n",
      "120000 : done\n",
      "130000 : done\n",
      "140000 : done\n",
      "150000 : done\n",
      "151402\n"
     ]
    }
   ],
   "source": [
    "text_neg_val = []\n",
    "text_neg_train = []\n",
    "text_neg_test = []\n",
    "count = 0\n",
    "count1 = 0\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR+\"/not_protest\")):\n",
    "    path = os.path.join(TEXT_DATA_DIR+'/not_protest', name)\n",
    "    f = open(path)\n",
    "    t = f.read()\n",
    "    t = t.split(\"</DOC>\")[:-1]\n",
    "    for i,docs in enumerate(t):\n",
    "        t[i] = re.sub(r\"((\\<.*\\>)|(\\d+)|(LEAD:(.*)\\n))\",\"\",t[i])\n",
    "        if (count%10000)==0:\n",
    "            print (count,\": done\")\n",
    "        if count in test_sample_indices:\n",
    "            text_neg_test.append(t[i])\n",
    "        elif count in val_sample_indices:\n",
    "            text_neg_val.append(t[i])\n",
    "        elif count in train_sample_indices:\n",
    "            text_neg_train.append(t[i])\n",
    "        count=count+1\n",
    "    f.close()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1645\n"
     ]
    }
   ],
   "source": [
    "text_pos_val = []\n",
    "text_pos_train  = []\n",
    "text_pos_test = []\n",
    "count = 0\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR+\"/protest\")):\n",
    "    path = os.path.join(TEXT_DATA_DIR+'/protest', name)\n",
    "    f = open(path)\n",
    "    t = f.read()\n",
    "    t = t.split(\"</DOC>\")[:-1]\n",
    "    for i,docs in enumerate(t):\n",
    "        t[i] = re.sub(r\"((\\<.*\\>)|(\\d+)|(LEAD:(.*)\\n))\",\"\",t[i])\n",
    "        if count in test_sample_indices_protest:\n",
    "            text_pos_test.append(t[i])\n",
    "        elif count in val_sample_indices_protest:\n",
    "            text_pos_val.append(t[i])\n",
    "        elif count in train_sample_indices_protest:\n",
    "            text_pos_train.append(t[i])\n",
    "        count= count +1\n",
    "    f.close()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_data  =  text_neg_train + text_pos_train + text_neg_val + text_pos_val\n",
    "\n",
    "label_neg_train = np.zeros((len(text_neg_train),),dtype=int)\n",
    "label_pos_train = np.ones((len(text_pos_train),),dtype=int)\n",
    "labels_train = np.append(label_neg_train,label_pos_train)\n",
    "\n",
    "label_neg_val = np.zeros((len(text_neg_val),),dtype=int)\n",
    "label_pos_val = np.ones((len(text_pos_val),),dtype=int)\n",
    "labels_val = np.append(label_neg_val,label_pos_val)\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer,pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "MAX_NB_WORDS =50000\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "\n",
    "x_train = data[:len(text_neg_train)+len(text_pos_train)]\n",
    "x_val = data[len(text_neg_train)+len(text_pos_train):]\n",
    "\n",
    "y_train = to_categorical(np.asarray(labels_train))\n",
    "y_val = to_categorical(np.asarray(labels_val))\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(text_pos_test)\n",
    "x_test_pos = pad_sequences(test_seq,maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "y_test_positives = np.ones((len(text_pos_test),))\n",
    "temp = np.append(np.zeros((1,)),y_test_positives)\n",
    "y_test_positives = to_categorical(temp)[1:]\n",
    "\n",
    "test_seq_ = tokenizer.texts_to_sequences(text_neg_test)\n",
    "x_test_neg = pad_sequences(test_seq_,maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "y_test_negatives = np.zeros((len(text_neg_test),))\n",
    "temp = np.append(np.ones((1,)),y_test_negatives)\n",
    "y_test_negatives = to_categorical(temp)[1:]\n",
    "\n",
    "x_test = np.append(x_test_neg,x_test_pos,axis=0)\n",
    "y_test = to_categorical(np.append(np.zeros((len(text_neg_test),)),np.ones((len(text_pos_test),))))\n",
    "pd.to_pickle(obj=[x_test,y_test], path=fourth_model_dir + \"test_dataset\")\n",
    "\n",
    "x_train_neg = data[:106402]\n",
    "\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(x_train_neg)\n",
    "\n",
    "x_train_neg_10 = x_train_neg[:12000]\n",
    "x_train_neg_1 = x_train_neg[:1200]\n",
    "\n",
    "x_train_pos = data[106402:106402+1150]\n",
    "\n",
    "x_train = np.append(x_train_neg_10,x_train_pos,axis = 0)\n",
    "y_train = to_categorical(np.append(np.zeros(12000,),np.ones(1150,),axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "tokenizer = pd.read_pickle(\"/home/abhinav/data/fourth_model/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"/home/abhinav/data/fourth_model/data_in_sequence_form\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle(obj=tokenizer,path = \"/home/abhinav/data/fourth_model/tokenizer\")\n",
    "pd.to_pickle(obj=data, path = \"/home/abhinav/data/fourth_model/data_in_sequence_form\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fourth_model_dir = \"/home/abhinav/data/fourth_model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle(obj=[x_val, y_val], path= fourth_model_dir + \"validation_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to convert downsample the non protest data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_neg = data[:106402]\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(x_train_neg)\n",
    "\n",
    "\n",
    "x_train_neg_10 = x_train_neg[:12000]\n",
    "x_train_neg_1 = x_train_neg[:1200]\n",
    "\n",
    "\n",
    "\n",
    "x_train_pos = data[106402:106402+1150]\n",
    "\n",
    "x_train = np.append(x_train_neg_10,x_train_pos,axis = 0)\n",
    "y_train = to_categorical(np.append(np.zeros(12000,),np.ones(1150,),axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle([x_train_pos,y_train_pos],\"/home/abhinav/data/second_model/train_positive_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to create the traininig data after downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle([x_train,y_train],\"/home/abhinav/data/fourth_model/train_data_10ratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.to_pickle(obj=[x_train,y_train],path=second_model_dir+\"training_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to get only the protest examples in the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_val_positives = data[-330:]\n",
    "y_val_positives = np.ones((330,))\n",
    "temp = np.append(np.zeros((1,)),y_val_positives)\n",
    "y_val_positives = to_categorical(temp)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle([x_val_positives,y_val_positives],path = \"/home/abhinav/data/second_model/val_positive_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to get only the non protest examples of the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_val_negatives = data[-30330:-330]\n",
    "y_val_negatives = np.zeros((30000,))\n",
    "temp_ = np.append(np.ones((1,)),y_val_negatives)\n",
    "y_val_negatives = to_categorical(temp_)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle(obj = [x_val_negatives,y_val_negatives] , path=\"/home/abhinav/data/fourth_model/val_negative_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to get the positives & negatives from the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(text_pos_test)\n",
    "x_test_pos = pad_sequences(test_seq,maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "y_test_positives = np.ones((len(text_pos_test),))\n",
    "temp = np.append(np.zeros((1,)),y_test_positives)\n",
    "y_test_positives = to_categorical(temp)[1:]\n",
    "\n",
    "test_seq_ = tokenizer.texts_to_sequences(text_neg_test)\n",
    "x_test_neg = pad_sequences(test_seq_,maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "y_test_negatives = np.zeros((len(text_neg_test),))\n",
    "temp = np.append(np.ones((1,)),y_test_negatives)\n",
    "y_test_negatives = to_categorical(temp)[1:]\n",
    "\n",
    "x_test = np.append(x_test_neg,x_test_pos,axis=0)\n",
    "y_test = to_categorical(np.append(np.zeros((len(text_neg_test),)),np.ones((len(text_pos_test),))))\n",
    "pd.to_pickle(obj=[x_test,y_test], path=fourth_model_dir + \"test_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.to_pickle(embedding_matrix, fourth_model_dir + \"embedding_matrix_for_selected_words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indice_shuffler = np.arange(x_train.shape[0])\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(indice_shuffler)\n",
    "x_train = x_train[indice_shuffler]\n",
    "y_train = y_train[indice_shuffler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13150, 1000)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indice_shuffler_ = np.arange(x_val.shape[0])\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(indice_shuffler_)\n",
    "x_val = x_val[indice_shuffler_]\n",
    "y_val = y_val[indice_shuffler_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pretrained = False\n",
    "if pretrained:\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Metrics(keras.callbacks.Callback):\n",
    "    def __init__(self,filepath):\n",
    "        self.filepath = filepath\n",
    "        self.best = -np.Inf\n",
    "    def on_epoch_end(self, epoch,batch, logs={}):\n",
    "        predict = to_categorical(self.model.predict_classes(self.validation_data[0],batch_size = 512))\n",
    "        targ = self.validation_data[1]\n",
    "        self.f1s=f1(targ, predict)\n",
    "        \"\"\"if self.f1s > self.best:\n",
    "            self.best = self.f1s\n",
    "            print('Epoch %05d: improved f1 to %0.5f,'\n",
    "                                  ' saving model to %s'\n",
    "                                  % (epoch, self.best, self.filepath))\n",
    "            self.model.save(self.filepath,overwrite =True)\"\"\"\n",
    "        print (\"Weighted F1 score found on Validation dataset : \" ,self.f1s)\n",
    "        return\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "def f1(y_true, y_pred):\n",
    "    return precision_recall_fscore_support(y_true = y_true, y_pred = y_pred)\n",
    "ndm = keras.optimizers.Nadam(lr = 0.001)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_ = Sequential()\n",
    "model_.add(Embedding(num_words,EMBEDDING_DIM,input_length=MAX_SEQUENCE_LENGTH))\n",
    "model_.add(Conv1D(128, 5, activation='relu'))\n",
    "model_.add(MaxPooling1D(5))\n",
    "model_.add(Conv1D(128, 5, activation='relu'))\n",
    "model_.add(MaxPooling1D(5))\n",
    "model_.add(Conv1D(128, 5, activation='relu'))\n",
    "model_.add(MaxPooling1D(35))\n",
    "model_.add(Flatten())\n",
    "model_.add(Dense(128,activation=\"relu\"))\n",
    "model_.add(Dense(2,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_.compile(loss = \"binary_crossentropy\",metrics = [\"acc\"], \n",
    "                       optimizer=ndm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_model_weights = model_.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_.set_weights(initial_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13150 samples, validate on 15165 samples\n",
      "Epoch 1/4\n",
      "15165/15165 [==============================] - 91s    \n",
      "Weighted F1 score found on Validation dataset :  (array([ 0.99252811,  0.09705373]), array([ 0.96526667,  0.33939394]), array([ 0.97870758,  0.1509434 ]), array([15000,   165]))\n",
      "13150/13150 [==============================] - 407s - loss: 0.9513 - acc: 0.8644 - val_loss: 0.1606 - val_acc: 0.9585\n",
      "Epoch 2/4\n",
      "15165/15165 [==============================] - 91s    \n",
      "Weighted F1 score found on Validation dataset :  (array([ 0.9961544 ,  0.08098337]), array([ 0.91526667,  0.67878788]), array([ 0.95399903,  0.14470284]), array([15000,   165]))\n",
      "13150/13150 [==============================] - 403s - loss: 0.5391 - acc: 0.8649 - val_loss: 0.2138 - val_acc: 0.9127\n",
      "Epoch 3/4\n",
      "15165/15165 [==============================] - 86s    \n",
      "Weighted F1 score found on Validation dataset :  (array([ 0.99643273,  0.05242291]), array([ 0.8566    ,  0.72121212]), array([ 0.92124037,  0.09774127]), array([15000,   165]))\n",
      "13150/13150 [==============================] - 399s - loss: 0.2411 - acc: 0.9418 - val_loss: 0.3434 - val_acc: 0.8551\n",
      "Epoch 4/4\n",
      "15165/15165 [==============================] - 86s    \n",
      "Weighted F1 score found on Validation dataset :  (array([ 0.99454326,  0.08349146]), array([ 0.9356    ,  0.53333333]), array([ 0.96417162,  0.14438064]), array([15000,   165]))\n",
      "13150/13150 [==============================] - 390s - loss: 0.0843 - acc: 0.9792 - val_loss: 0.2238 - val_acc: 0.9312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f73b1693a90>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = Metrics(fourth_model_dir + \"none\")\n",
    "model_.fit(x_train,y_train\n",
    "                  ,batch_size = 128,\n",
    "                  epochs = 4,\n",
    "                  class_weight = {0. :1, 1. : 7},\n",
    "                  validation_data = [x_test,y_test],\n",
    "                   callbacks = [metrics]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_.set_weights(initial_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13150 samples, validate on 15165 samples\n",
      "Epoch 1/4\n",
      "15165/15165 [==============================] - 86s    \n",
      "Weighted F1 score found on Validation dataset :  (array([ 0.99818182,  0.0178634 ]), array([ 0.4392    ,  0.92727273]), array([ 0.61      ,  0.03505155]), array([15000,   165]))\n",
      "13150/13150 [==============================] - 389s - loss: 1.0563 - acc: 0.8143 - val_loss: 0.7295 - val_acc: 0.4445\n",
      "Epoch 2/4\n",
      "15165/15165 [==============================] - 87s    \n",
      "Weighted F1 score found on Validation dataset :  (array([ 0.9984148 ,  0.03858268]), array([ 0.7558    ,  0.89090909]), array([ 0.86033011,  0.07396226]), array([15000,   165]))\n",
      "13150/13150 [==============================] - 389s - loss: 0.5849 - acc: 0.8281 - val_loss: 0.5090 - val_acc: 0.7573\n",
      "Epoch 3/4\n",
      "15165/15165 [==============================] - 82s    \n",
      "Weighted F1 score found on Validation dataset :  (array([ 0.99557869,  0.09019264]), array([ 0.93073333,  0.62424242]), array([ 0.96206457,  0.15761285]), array([15000,   165]))\n",
      "13150/13150 [==============================] - 385s - loss: 0.2016 - acc: 0.9544 - val_loss: 0.2032 - val_acc: 0.9274\n",
      "Epoch 4/4\n",
      "15165/15165 [==============================] - 82s    \n",
      "Weighted F1 score found on Validation dataset :  (array([ 0.99391536,  0.14126394]), array([ 0.9692    ,  0.46060606]), array([ 0.9814021 ,  0.21621622]), array([15000,   165]))\n",
      "13150/13150 [==============================] - 384s - loss: 0.0612 - acc: 0.9874 - val_loss: 0.1352 - val_acc: 0.9637\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f73b1693f50>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = Metrics(fourth_model_dir + \"none\")\n",
    "model_.fit(x_train,y_train\n",
    "                  ,batch_size = 128,\n",
    "                  epochs = 4,\n",
    "                  class_weight = {0. :1, 1. : 8},\n",
    "                  validation_data = [x_test,y_test],\n",
    "                   callbacks = [metrics]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_.set_weights(initial_model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13150 samples, validate on 30330 samples\n",
      "Epoch 1/4\n",
      "30330/30330 [==============================] - 173s   \n",
      "Weighted F1 score found on Validation dataset :  (array([ 0.9945685 ,  0.04871661]), array([ 0.87893333,  0.56363636]), array([ 0.93318233,  0.08968177]), array([30000,   330]))\n",
      "13150/13150 [==============================] - 558s - loss: 1.1628 - acc: 0.6973 - val_loss: 0.3204 - val_acc: 0.8755\n",
      "Epoch 2/4\n",
      "30330/30330 [==============================] - 172s   \n",
      "Weighted F1 score found on Validation dataset :  (array([ 0.99604775,  0.04374172]), array([ 0.83166667,  0.7       ]), array([ 0.90646515,  0.08233826]), array([30000,   330]))\n",
      "13150/13150 [==============================] - 557s - loss: 0.6770 - acc: 0.8260 - val_loss: 0.3724 - val_acc: 0.8302\n",
      "Epoch 3/4\n",
      "30330/30330 [==============================] - 170s   \n",
      "Weighted F1 score found on Validation dataset :  (array([ 0.99465024,  0.05734656]), array([ 0.89863333,  0.56060606]), array([ 0.94420706,  0.10404949]), array([30000,   330]))\n",
      "13150/13150 [==============================] - 556s - loss: 0.2474 - acc: 0.9428 - val_loss: 0.2673 - val_acc: 0.8950\n",
      "Epoch 4/4\n",
      "30330/30330 [==============================] - 170s   \n",
      "Weighted F1 score found on Validation dataset :  (array([ 0.99307503,  0.08971705]), array([ 0.95603333,  0.39393939]), array([ 0.9742022 ,  0.14614952]), array([30000,   330]))\n",
      "13150/13150 [==============================] - 555s - loss: 0.0864 - acc: 0.9827 - val_loss: 0.1659 - val_acc: 0.9499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f73b4fa2190>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_.fit(x_train,y_train\n",
    "                  ,batch_size = 128,\n",
    "                  epochs =4,\n",
    "                  class_weight = {0. :1, 1. : 10},\n",
    "                  validation_data = [x_val,y_val],\n",
    "                   callbacks = [metrics]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model1 = load_model(\"/home/abhinav/data/second_model/the_best_model_with_trained_word_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13150 samples, validate on 30330 samples\n",
      "Epoch 1/2\n",
      " 3072/30330 [==>...........................] - ETA: 294s loss: 0.0314 - a"
     ]
    }
   ],
   "source": [
    "model_.fit(x_train,y_train\n",
    "                  ,batch_size = 128,\n",
    "                  epochs =2,\n",
    "                  class_weight = {0. :1, 1. : 10},\n",
    "                  validation_data = [x_val,y_val],\n",
    "                   callbacks = [metrics]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/30000 [==============================] - 173s   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7458493810017903, 0.11363333338101705]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_.evaluate(x_val_negatives,y_val_negatives,batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165/165 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.072426959872245789, 1.0]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_.evaluate(x_test_pos,y_test_positives,batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000 [==============================] - 85s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7384219600677491, 0.11386666659514109]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_.evaluate(x_test_neg,y_test_negatives,batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_1 = load_model(second_model_dir+\"lets_try_again_to_save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.85618859529495239, 0.691428542137146]"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(x_test_pos,y_test_positives,batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 1s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.88645219734736846, 0.69714285884584704]"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(x_val_positives,y_val_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/30000 [==============================] - 105s   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15647533753712972, 0.93570000009536747]"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(x_val_negatives,y_val_negatives,batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.],\n",
       "       [ 0.,  1.]])"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(model_.predict_classes(x_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='binary_crossentropy',metrics=[\"acc\"]\n",
    "              ,optimizer=ndm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_weights = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.set_weights(initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11228 samples, validate on 30350 samples\n",
      "Epoch 1/2\n",
      "11136/11228 [============================>.] - ETA: 1s - loss: 1.2254 - acc: 0.6608Epoch 00000: improved f1 to 0.98274, saving model to /home/abhinav/data/second_model/trying_to_save_model\n",
      "Weighted F1 score found on Validation dataset :  0.982735252488\n",
      "11228/11228 [==============================] - 334s - loss: 1.2240 - acc: 0.6630 - val_loss: 0.6886 - val_acc: 0.9885\n",
      "Epoch 2/2\n",
      "11136/11228 [============================>.] - ETA: 1s - loss: 1.2226 - acc: 0.8909Weighted F1 score found on Validation dataset :  0.982735252488\n",
      "11228/11228 [==============================] - 323s - loss: 1.2239 - acc: 0.8906 - val_loss: 0.6867 - val_acc: 0.9885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb7da9aef90>"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = Metrics(second_model_dir + \"trying_to_save_model\")\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=2,\n",
    "          class_weight = { 0. : 1, 1. : 8 },\n",
    "          validation_data=(x_val, y_val),\n",
    "          callbacks =[metrics]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98273525248788973"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98273525248788973"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(x_val,y_val,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "second_model_dir = \"/home/abhinav/data/second_model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1 = load_model(second_model_dir + \"saved_model_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 1s\n",
      "[0.57591825723648071, 0.96285712718963623]\n",
      "30000/30000 [==============================] - 102s   \n",
      "[0.72436426213582361, 0.32140000011126202]\n"
     ]
    }
   ],
   "source": [
    "print (model1.evaluate(x_val_positives,y_val_positives,batch_size=512))\n",
    "print (model1.evaluate(x_val_negatives,y_val_negatives,batch_size=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.69980263710021973, 0.0]"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_val_positives,y_val_positives,batch_size=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000/30000 [==============================] - 93s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.68653606402079259, 1.0]"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_val_negatives,y_val_negatives,batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175/175 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.69980294534138265, 0.0]"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test_pos,y_test_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(x_test_neg,y_test_negatives,batch_size = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate(x_val[:100],y_val[:100],model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "del model\n",
    "model = load_model(\"/home/abhinav/data/second_model/the_best_model_with_trained_word_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"/home/abhinav/data/second_model/test_from_news\")\n",
    "\n",
    "test_news = f.read()\n",
    "\n",
    "test_news = test_news.split(\"</DOC>\")\n",
    "\n",
    "x_news = tokenizer.texts_to_sequences(test_news)\n",
    "\n",
    "x_news = pad_sequences(x_news,maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2799862 ,  0.7200138 ],\n",
       "       [ 0.53245479,  0.46754521]], dtype=float32)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=[X_test,y_test], \n",
    "       verbose=1, callbacks=[metrics])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
