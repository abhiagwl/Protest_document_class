{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences \n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_csv = pd.read_csv('/home/abhinav/data/full_text/alan_cleaner/filter_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np_test = data_csv[(data_csv.label == 0) & (data_csv.is_protest == False)]\n",
    "np_test_tags = np_test.filename.values\n",
    "np.random.seed(0)\n",
    "np_test_tags = shuffle(np_test_tags)[:45000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np_train_filter = data_csv[(data_csv.label == 0)&(data_csv.is_protest == True)] \n",
    "np_train_tags = np_train_filter.filename.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_train = data_csv[data_csv.label==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "p_train = shuffle(p_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_test = p_train[:500]\n",
    "p_train = p_train[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_train_tags = p_train.filename.values\n",
    "p_test_tags = p_test.filename.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, ' Done')\n",
      "(10000, ' Done')\n",
      "(20000, ' Done')\n",
      "(30000, ' Done')\n",
      "(40000, ' Done')\n",
      "(50000, ' Done')\n",
      "(60000, ' Done')\n",
      "(70000, ' Done')\n",
      "(80000, ' Done')\n",
      "(90000, ' Done')\n",
      "(100000, ' Done')\n",
      "(110000, ' Done')\n",
      "(120000, ' Done')\n",
      "(130000, ' Done')\n",
      "(140000, ' Done')\n",
      "(150000, ' Done')\n"
     ]
    }
   ],
   "source": [
    "TEXT_DATA_DIR = \"/home/abhinav/data/full_text\"\n",
    "text_neg_train = []\n",
    "text_neg_test = []\n",
    "count = 0\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR+\"/not_protest\")):\n",
    "    path = os.path.join(TEXT_DATA_DIR+'/not_protest', name)\n",
    "    f = open(path)\n",
    "    t = f.read()\n",
    "    t = t.split(\"</DOC>\")[:-1]\n",
    "    for i,docs in enumerate(t):\n",
    "        id_ = re.search(r\"id=\\\"(.*)\\\" type\",t[i]).group(1)[-7:]+\".txt\"\n",
    "        if (count %10000) == 0:\n",
    "            print (count,\" Done\")\n",
    "        if id_ in np_train_tags:\n",
    "            t[i] = re.sub(r\"((\\<.*\\>)|(\\d+)|(LEAD:(.*)\\n))\",\"\",t[i])\n",
    "            t[i] = [j for j in t[i].lower().split() if j not in stop]\n",
    "            t[i] = \" \".join(t[i])\n",
    "            t[i] = \" \".join(text_to_word_sequence(t[i]))\n",
    "            text_neg_train.append(t[i])\n",
    "        elif id_ in np_test_tags:\n",
    "            t[i] = re.sub(r\"((\\<.*\\>)|(\\d+)|(LEAD:(.*)\\n))\",\"\",t[i])\n",
    "            t[i] = [j for j in t[i].lower().split() if j not in stop]\n",
    "            t[i] = \" \".join(t[i])\n",
    "            t[i] = \" \".join(text_to_word_sequence(t[i]))\n",
    "            text_neg_test.append(t[i])\n",
    "        count = count +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEXT_DATA_DIR = \"/home/abhinav/data/full_text\"\n",
    "text_pos_train = []\n",
    "text_pos_test = []\n",
    "\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR+\"/protest\")):\n",
    "    path = os.path.join(TEXT_DATA_DIR+'/protest', name)\n",
    "    f = open(path)\n",
    "    t = f.read()\n",
    "    t = t.split(\"</DOC>\")[:-1]\n",
    "    for i,docs in enumerate(t):\n",
    "        id_ = re.search(r\"id=\\\"(.*)\\\" type\",t[i]).group(1)[-7:]+\".txt\"        \n",
    "        if id_ in p_train_tags:\n",
    "            t[i] = re.sub(r\"((\\<.*\\>)|(\\d+)|(LEAD:(.*)\\n))\",\"\",t[i])\n",
    "            t[i] = [j for j in t[i].lower().split() if j not in stop]\n",
    "            t[i] = \" \".join(t[i])\n",
    "            t[i] = \" \".join(text_to_word_sequence(t[i]))\n",
    "            text_pos_train.append(t[i])\n",
    "        elif id_ in p_test_tags:\n",
    "            t[i] = re.sub(r\"((\\<.*\\>)|(\\d+)|(LEAD:(.*)\\n))\",\"\",t[i])\n",
    "            t[i] = [j for j in t[i].lower().split() if j not in stop]\n",
    "            t[i] = \" \".join(t[i])\n",
    "            t[i] = \" \".join(text_to_word_sequence(t[i]))\n",
    "            text_pos_test.append(t[i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SIXTH_MOD_DIR = \"/home/abhinav/data/sixth_model_alan_filtering/\"\n",
    "MOD_DIR = SIXTH_MOD_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = '/home/abhinav/data/GLOVE_DATA/glove.6B'\n",
    "TEXT_DATA_DIR = '/home/abhinav/data/full_text'\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 50000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_data  =  text_neg_train + text_pos_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_pickle(tokenizer,MOD_DIR+'tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 103916 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of data tensor:', x_train.shape)\n",
    "\n",
    "label_neg_train = np.zeros((len(text_neg_train),),dtype=int)\n",
    "label_pos_train = np.ones((len(text_pos_train),),dtype=int)\n",
    "labels_train = np.append(label_neg_train,label_pos_train)\n",
    "y_train = to_categorical(np.asarray(labels_train))\n",
    "\n",
    "pd.to_pickle([x_train,y_train],MOD_DIR+'train_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(text_pos_test)\n",
    "x_test_pos = pad_sequences(test_seq,maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "y_test_positives = np.ones((len(text_pos_test),))\n",
    "temp = np.append(np.zeros((1,)),y_test_positives)\n",
    "y_test_positives = to_categorical(temp)[1:]\n",
    "\n",
    "test_seq_ = tokenizer.texts_to_sequences(text_neg_test)\n",
    "x_test_neg = pad_sequences(test_seq_,maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "y_test_negatives = np.zeros((len(text_neg_test),))\n",
    "temp = np.append(np.ones((1,)),y_test_negatives)\n",
    "y_test_negatives = to_categorical(temp)[1:]\n",
    "\n",
    "x_test = np.append(x_test_neg,x_test_pos,axis=0)\n",
    "y_test = to_categorical(np.append(np.zeros((len(text_neg_test),)),np.ones((len(text_pos_test),))))\n",
    "\n",
    "\n",
    "\n",
    "pd.to_pickle(obj=[x_test,y_test], path=MOD_DIR + \"test_dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors.')\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "#embeddings_index = pd.read_pickle('/home/abhinav/data/fourth_model/embedding_hash_300d')\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "pd.to_pickle(embedding_matrix,MOD_DIR + \"embedding_matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
